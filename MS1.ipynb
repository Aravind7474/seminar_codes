{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrg4XaudQJVsM9zHU+m7ZV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aravind7474/seminar_codes/blob/main/MS1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S9qAiS2r8bNZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "306e8047-a621-476e-8f54-eacc70efaaae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Encoder definition\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels=1, hidden_channels=64, latent_dim=64):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels, latent_dim, kernel_size=3, stride=1, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "# Decoder definition\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim=64, hidden_channels=64, out_channels=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(latent_dim, hidden_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(hidden_channels, hidden_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(hidden_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Sigmoid()  # Scale output between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.deconv(x)\n",
        "\n",
        "# Vector Quantizer for VQ-VAE\n",
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost=0.25):\n",
        "        super(VectorQuantizer, self).__init__()\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.commitment_cost = commitment_cost\n",
        "\n",
        "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.embeddings.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Flatten input: (B, latent_dim, H, W) -> (B*H*W, latent_dim)\n",
        "        input_shape = inputs.shape\n",
        "        flat_input = inputs.view(-1, self.embedding_dim)\n",
        "\n",
        "        # Compute distances between flat_input and embeddings\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
        "                     + torch.sum(self.embeddings.weight**2, dim=1)\n",
        "                     - 2 * torch.matmul(flat_input, self.embeddings.weight.t()))\n",
        "\n",
        "        # Find nearest embeddings\n",
        "        encoding_indices = torch.argmin(distances, dim=1)\n",
        "        encodings = torch.nn.functional.one_hot(encoding_indices, self.num_embeddings).float()\n",
        "\n",
        "        # Quantize and reshape back to input shape\n",
        "        quantized = torch.matmul(encodings, self.embeddings.weight).view(input_shape)\n",
        "\n",
        "        # Compute commitment losses\n",
        "        e_latent_loss = torch.mean((quantized.detach() - inputs)**2)\n",
        "        q_latent_loss = torch.mean((quantized - inputs.detach())**2)\n",
        "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
        "\n",
        "        # Straight Through Estimator\n",
        "        quantized = inputs + (quantized - inputs).detach()\n",
        "\n",
        "        return quantized, loss\n",
        "\n",
        "# VQ-VAE model that combines encoder, vector quantizer, and decoder\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, in_channels=1, hidden_channels=64, latent_dim=64, num_embeddings=128, commitment_cost=0.25):\n",
        "        super(VQVAE, self).__init__()\n",
        "        self.encoder = Encoder(in_channels, hidden_channels, latent_dim)\n",
        "        self.vector_quantizer = VectorQuantizer(num_embeddings, latent_dim, commitment_cost)\n",
        "        self.decoder = Decoder(latent_dim, hidden_channels, in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z_e = self.encoder(x)\n",
        "        z_q, vq_loss = self.vector_quantizer(z_e)\n",
        "        x_recon = self.decoder(z_q)\n",
        "        return x_recon, vq_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Encoder definition\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels=1, hidden_channels=64, latent_dim=64):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels, latent_dim, kernel_size=3, stride=1, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "# Decoder definition\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim=64, hidden_channels=64, out_channels=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(latent_dim, hidden_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(hidden_channels, hidden_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(hidden_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Sigmoid()  # Scale output between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.deconv(x)\n",
        "\n",
        "# Vector Quantizer for VQ-VAE\n",
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost=0.25):\n",
        "        super(VectorQuantizer, self).__init__()\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.commitment_cost = commitment_cost\n",
        "\n",
        "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.embeddings.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Flatten input: (B, latent_dim, H, W) -> (B*H*W, latent_dim)\n",
        "        input_shape = inputs.shape\n",
        "        flat_input = inputs.view(-1, self.embedding_dim)\n",
        "\n",
        "        # Compute distances between flat_input and embeddings\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
        "                     + torch.sum(self.embeddings.weight**2, dim=1)\n",
        "                     - 2 * torch.matmul(flat_input, self.embeddings.weight.t()))\n",
        "\n",
        "        # Find nearest embeddings\n",
        "        encoding_indices = torch.argmin(distances, dim=1)\n",
        "        encodings = torch.nn.functional.one_hot(encoding_indices, self.num_embeddings).float()\n",
        "\n",
        "        # Quantize and reshape back to input shape\n",
        "        quantized = torch.matmul(encodings, self.embeddings.weight).view(input_shape)\n",
        "\n",
        "        # Compute commitment losses\n",
        "        e_latent_loss = torch.mean((quantized.detach() - inputs)**2)\n",
        "        q_latent_loss = torch.mean((quantized - inputs.detach())**2)\n",
        "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
        "\n",
        "        # Straight Through Estimator\n",
        "        quantized = inputs + (quantized - inputs).detach()\n",
        "\n",
        "        return quantized, loss\n",
        "\n",
        "# VQ-VAE model that combines encoder, vector quantizer, and decoder\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, in_channels=1, hidden_channels=64, latent_dim=64, num_embeddings=128, commitment_cost=0.25):\n",
        "        super(VQVAE, self).__init__()\n",
        "        self.encoder = Encoder(in_channels, hidden_channels, latent_dim)\n",
        "        self.vector_quantizer = VectorQuantizer(num_embeddings, latent_dim, commitment_cost)\n",
        "        self.decoder = Decoder(latent_dim, hidden_channels, in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z_e = self.encoder(x)\n",
        "        z_q, vq_loss = self.vector_quantizer(z_e)\n",
        "        x_recon = self.decoder(z_q)\n",
        "        return x_recon, vq_loss\n"
      ],
      "metadata": {
        "id": "_Y6EfCghQZbH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sample_dataset(num_samples=1000, channels=1, height=32, width=32):\n",
        "    \"\"\"\n",
        "    Simulate a dataset of 2D histograms.\n",
        "    In a real scenario, replace this with your preprocessing converting\n",
        "    spatio-temporal data into the required format.\n",
        "    \"\"\"\n",
        "    data = torch.rand(num_samples, channels, height, width)\n",
        "    return data\n",
        "\n",
        "# Create and wrap the dataset in a DataLoader\n",
        "data = create_sample_dataset(num_samples=1000, channels=1, height=32, width=32)\n",
        "dataset = TensorDataset(data)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "9KtvzSiSQnoK"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}